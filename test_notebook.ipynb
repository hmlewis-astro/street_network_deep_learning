{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_notebook.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJkrQiPilMVtuaLa0kSBcf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmlewis-astro/street_network_deep_learning/blob/main/test_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yzjpQAofZ1d"
      },
      "source": [
        "# Change to GPU runtime\n",
        "\n",
        "#### Navigate to \"Runtime > Change runtime type > GPU > Save\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcnn78jeIQP"
      },
      "source": [
        "# Download Kaggle API credentials\n",
        "#### **Note**: This is a one-time step and you donâ€™t need to generate the credentials every time you download the dataset.\n",
        "- Navigate to your Kaggle profile\n",
        "- Click the \"Account\" tab\n",
        "- Scroll down to the \"API\" section\n",
        "- Click \"Create New API Token\"; a file named `kaggle.json` will be download which contains your username and API key\n",
        "\n",
        "# Upload Kaggle API credentials to Google Colab\n",
        "#### **Note**: Uploaded files will get deleted when this runtime is recycled.\n",
        "- Upload the `kaggle.json` file that you just downloaded from Kaggle\n",
        "- Run the following cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6sWzwKcbPxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c234aa85-c72d-48ca-958b-e60a56ee0cdb"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download balraj98/deepglobe-road-extraction-dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading deepglobe-road-extraction-dataset.zip to /content\n",
            "100% 3.79G/3.79G [01:08<00:00, 53.2MB/s]\n",
            "100% 3.79G/3.79G [01:08<00:00, 59.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztl-MeS9bVH4"
      },
      "source": [
        "!unzip -q /content/deepglobe-road-extraction-dataset.zip -d /content/deep-globe"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ459vgNgLTT"
      },
      "source": [
        "!rm -rf /content/deepglobe-road-extraction-dataset.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klEmSpOaF5ag",
        "outputId": "f2db6e40-d5fa-4df9-f6a9-c5ff059a2fd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!pip install git+https://github.com/tensorflow/examples.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/tensorflow/examples.git\n",
            "  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-549ywo5u\n",
            "  Running command git clone -q https://github.com/tensorflow/examples.git /tmp/pip-req-build-549ywo5u\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-examples===079eae91b01d7666471c9e01dadd031e2c2a00f2-) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-examples===079eae91b01d7666471c9e01dadd031e2c2a00f2-) (1.15.0)\n",
            "Building wheels for collected packages: tensorflow-examples\n",
            "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-examples: filename=tensorflow_examples-079eae91b01d7666471c9e01dadd031e2c2a00f2_-py3-none-any.whl size=271371 sha256=b2b42a5a14e4404956e05b2f726a941c3ec5e168fb2403037cc38570c1e050f3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bc6mrfi1/wheels/eb/19/50/2a4363c831fa12b400af86325a6f26ade5d2cdc5b406d552ca\n",
            "\u001b[33m  WARNING: Built wheel for tensorflow-examples is invalid: Metadata 1.2 mandates PEP 440 version, but '079eae91b01d7666471c9e01dadd031e2c2a00f2-' is not\u001b[0m\n",
            "Failed to build tensorflow-examples\n",
            "Installing collected packages: tensorflow-examples\n",
            "    Running setup.py install for tensorflow-examples ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: tensorflow-examples was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n",
            "Successfully installed tensorflow-examples-079eae91b01d7666471c9e01dadd031e2c2a00f2-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4dwawGDiTnF"
      },
      "source": [
        "# Import packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZxAjuatiXHQ"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "\n",
        "tqdm.pandas()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP69Bk14emJS",
        "outputId": "e03d8c34-8ca5-40c3-ad26-a0d6868ec3ef"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9m2J5A3esQJ"
      },
      "source": [
        "# Get class dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "XlD0waV7ijsZ",
        "outputId": "9d9411fa-8757-4b68-ed12-fd9d220c6b85"
      },
      "source": [
        "class_dict_path = \"/content/deep-globe/class_dict.csv\"\n",
        "class_dict = pd.read_csv(class_dict_path)\n",
        "class_names = class_dict['name'].tolist()\n",
        "class_rgb_values = class_dict[['r','g','b']].values.tolist()\n",
        "class_dict"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>r</th>\n",
              "      <th>g</th>\n",
              "      <th>b</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>road</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>background</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         name    r    g    b\n",
              "0        road  255  255  255\n",
              "1  background    0    0    0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYix2ssUezQg"
      },
      "source": [
        "# Get metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tP52Rv99isSn",
        "outputId": "66bf0ad3-ff73-4894-ec17-e99da8a74821"
      },
      "source": [
        "metadata_path = \"/content/deep-globe/metadata.csv\"\n",
        "metadata = pd.read_csv(metadata_path)\n",
        "metadata.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>split</th>\n",
              "      <th>sat_image_path</th>\n",
              "      <th>mask_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100034</td>\n",
              "      <td>train</td>\n",
              "      <td>train/100034_sat.jpg</td>\n",
              "      <td>train/100034_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100081</td>\n",
              "      <td>train</td>\n",
              "      <td>train/100081_sat.jpg</td>\n",
              "      <td>train/100081_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100129</td>\n",
              "      <td>train</td>\n",
              "      <td>train/100129_sat.jpg</td>\n",
              "      <td>train/100129_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100703</td>\n",
              "      <td>train</td>\n",
              "      <td>train/100703_sat.jpg</td>\n",
              "      <td>train/100703_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100712</td>\n",
              "      <td>train</td>\n",
              "      <td>train/100712_sat.jpg</td>\n",
              "      <td>train/100712_mask.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id  split        sat_image_path              mask_path\n",
              "0    100034  train  train/100034_sat.jpg  train/100034_mask.png\n",
              "1    100081  train  train/100081_sat.jpg  train/100081_mask.png\n",
              "2    100129  train  train/100129_sat.jpg  train/100129_mask.png\n",
              "3    100703  train  train/100703_sat.jpg  train/100703_mask.png\n",
              "4    100712  train  train/100712_sat.jpg  train/100712_mask.png"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYcKUcXLe3je"
      },
      "source": [
        "### Get training/validation data (i.e., images with available road masks)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RuM5WVTTivJz",
        "outputId": "b06937b6-5796-499b-e532-d8914b869c95"
      },
      "source": [
        "metadata_train = metadata[metadata['split'] == 'train']\n",
        "metadata_train = metadata_train.drop('split', axis=1)\n",
        "metadata_train.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>sat_image_path</th>\n",
              "      <th>mask_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100034</td>\n",
              "      <td>train/100034_sat.jpg</td>\n",
              "      <td>train/100034_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100081</td>\n",
              "      <td>train/100081_sat.jpg</td>\n",
              "      <td>train/100081_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100129</td>\n",
              "      <td>train/100129_sat.jpg</td>\n",
              "      <td>train/100129_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100703</td>\n",
              "      <td>train/100703_sat.jpg</td>\n",
              "      <td>train/100703_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100712</td>\n",
              "      <td>train/100712_sat.jpg</td>\n",
              "      <td>train/100712_mask.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id        sat_image_path              mask_path\n",
              "0    100034  train/100034_sat.jpg  train/100034_mask.png\n",
              "1    100081  train/100081_sat.jpg  train/100081_mask.png\n",
              "2    100129  train/100129_sat.jpg  train/100129_mask.png\n",
              "3    100703  train/100703_sat.jpg  train/100703_mask.png\n",
              "4    100712  train/100712_sat.jpg  train/100712_mask.png"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNFqEIrCOdG6"
      },
      "source": [
        "metadata_train = metadata_train.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Woy3Du8clfwu",
        "outputId": "8093c3a9-1802-49e7-e8a8-29d0590a759d"
      },
      "source": [
        "metadata_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6226, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwlKV3VOfEh6"
      },
      "source": [
        "### Get test data (i.e., images without available road masks)\n",
        "\n",
        "Combine the datasets defined (by Kaggle) as \"validation\" and \"test\", because the \"validation\" set does not have road masks, so cannot actually be used for validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "o9tC9am-jmUq",
        "outputId": "406ebb02-2d36-4aa1-e9fa-0aa5517d0611"
      },
      "source": [
        "metadata_test = metadata[(metadata['split'] == 'valid') | \n",
        "                         (metadata['split'] == 'test')]\n",
        "metadata_test = metadata_test.drop(['split', 'mask_path'], axis=1)\n",
        "metadata_test.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>sat_image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6226</th>\n",
              "      <td>100794</td>\n",
              "      <td>valid/100794_sat.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6227</th>\n",
              "      <td>100905</td>\n",
              "      <td>valid/100905_sat.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6228</th>\n",
              "      <td>102867</td>\n",
              "      <td>valid/102867_sat.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6229</th>\n",
              "      <td>10417</td>\n",
              "      <td>valid/10417_sat.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6230</th>\n",
              "      <td>106553</td>\n",
              "      <td>valid/106553_sat.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      image_id        sat_image_path\n",
              "6226    100794  valid/100794_sat.jpg\n",
              "6227    100905  valid/100905_sat.jpg\n",
              "6228    102867  valid/102867_sat.jpg\n",
              "6229     10417   valid/10417_sat.jpg\n",
              "6230    106553  valid/106553_sat.jpg"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX_aYYw8Pug8"
      },
      "source": [
        "metadata_test = metadata_test.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehEKKQ4kljF7",
        "outputId": "cdd2765a-6731-4bb9-8ed8-eacc987104cb"
      },
      "source": [
        "metadata_test.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2344, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt5Sr7cLkstQ"
      },
      "source": [
        "data_path = \"/content/deep-globe/\"\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNTJT4Fhl2F0"
      },
      "source": [
        "metadata_train[\"sat_image_path\"] = metadata_train[\"sat_image_path\"] \\\n",
        "                                    .apply(lambda x: os.path.join(data_path, x))\n",
        "metadata_train[\"mask_path\"] = metadata_train[\"mask_path\"] \\\n",
        "                                    .apply(lambda x: os.path.join(data_path, x))\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlitFt7omNxo"
      },
      "source": [
        "metadata_test[\"sat_image_path\"] = metadata_test[\"sat_image_path\"] \\\n",
        "                                    .apply(lambda x: os.path.join(data_path, x))\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZHmHCbNnmcov",
        "outputId": "74e73eb9-37d1-4a89-b315-a094b1e3a9da"
      },
      "source": [
        "metadata_train.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>sat_image_path</th>\n",
              "      <th>mask_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>820820</td>\n",
              "      <td>/content/deep-globe/train/820820_sat.jpg</td>\n",
              "      <td>/content/deep-globe/train/820820_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>279620</td>\n",
              "      <td>/content/deep-globe/train/279620_sat.jpg</td>\n",
              "      <td>/content/deep-globe/train/279620_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>876221</td>\n",
              "      <td>/content/deep-globe/train/876221_sat.jpg</td>\n",
              "      <td>/content/deep-globe/train/876221_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>880871</td>\n",
              "      <td>/content/deep-globe/train/880871_sat.jpg</td>\n",
              "      <td>/content/deep-globe/train/880871_mask.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>873793</td>\n",
              "      <td>/content/deep-globe/train/873793_sat.jpg</td>\n",
              "      <td>/content/deep-globe/train/873793_mask.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id  ...                                  mask_path\n",
              "0    820820  ...  /content/deep-globe/train/820820_mask.png\n",
              "1    279620  ...  /content/deep-globe/train/279620_mask.png\n",
              "2    876221  ...  /content/deep-globe/train/876221_mask.png\n",
              "3    880871  ...  /content/deep-globe/train/880871_mask.png\n",
              "4    873793  ...  /content/deep-globe/train/873793_mask.png\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFK7fjYhEb6Q"
      },
      "source": [
        "class SatDatClass(tf.keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size)\n",
        "            x[j] = img_to_array(img).astype(int) / 255\n",
        "        y = np.zeros((self.batch_size,) + self.img_size,# + (1,),\n",
        "                     dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            target = load_img(path, target_size=self.img_size)#, \n",
        "                           #color_mode=\"grayscale\")\n",
        "            target = np.array(target).astype(int) / 255\n",
        "            y[j] = target[:, :, 0]\n",
        "            #y[j] = np.expand_dims(img, 2)\n",
        "            #y[j] = np.array(img)\n",
        "            #y[j] = np.expand_dims(img, 2).astype(int) // 255 #easier for network to interpret numbers in range [0,1]\n",
        "            # Ground truth labels are 1, 2, 3. \n",
        "            # Subtract one to make them 0, 1, 2:\n",
        "            #y[j] -= 1\n",
        "        \n",
        "        return x, y"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nyB7BbvlCJ2"
      },
      "source": [
        "class SatDatClass(tf.keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size)\n",
        "            x[j] = img\n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
        "            y[j] = np.expand_dims(img, 2)\n",
        "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "            y[j] -= 1\n",
        "        return x, y"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBxXgomjO5x0"
      },
      "source": [
        "train_dict = {'img' : [], 'mask' : []}\n",
        "\n",
        "def load_data(load_dict=None, input_img_paths=None, target_img_paths=None, image_size=(128, 128)):\n",
        "    image_names = os.listdir(input_img_paths)\n",
        "    target_names = []\n",
        "\n",
        "    for name in image_names:\n",
        "        name = name.split('_')[0]\n",
        "        if name not in target_names:\n",
        "            target_names.append(name)\n",
        "    \n",
        "    image_dir = input_img_paths + '/'\n",
        "    target_dir = target_img_paths + '/'\n",
        "    \n",
        "    for i in range (len(image_names)):\n",
        "        try:\n",
        "            img = plt.imread(image_dir + target_names[i] + '_sat.jpg') \n",
        "            target = plt.imread(target_dir + target_names[i] + '_mask.png')\n",
        "            \n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        img = cv2.resize(img, image_size)\n",
        "        target = cv2.resize(target, image_size)\n",
        "\n",
        "        load_dict['img'].append(img)\n",
        "        load_dict['mask'].append(target[:,:,0])\n",
        "        \n",
        "    return load_dict"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es-mv7cZPEXW"
      },
      "source": [
        "def Conv2DBlock(inputs, previous_block_activation, num_filters, kernel_size=3, batch_norm=True):\n",
        "    \n",
        "    x = layers.Activation('relu')(inputs)    \n",
        "\n",
        "    #x = layers.Conv2D(filters=num_filters, \n",
        "    x = layers.SeparableConv2D(filters=num_filters, \n",
        "                      kernel_size=(kernel_size, kernel_size),\n",
        "                      #kernel_initializer='he_normal', \n",
        "                      padding='same')(x)\n",
        "    \n",
        "    if batch_norm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Activation('relu')(x)    \n",
        "    \n",
        "    #x = tf.keras.layers.Conv2D(filters=num_filters, \n",
        "    x = layers.SeparableConv2D(filters=num_filters, \n",
        "                               kernel_size=(kernel_size, kernel_size),\n",
        "                               #kernel_initializer='he_normal', \n",
        "                               padding='same') (x)\n",
        "    if batch_norm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    return x\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmEJMeIGPEVa"
      },
      "source": [
        "def Conv2DTransposeBlock(inputs, previous_block_activation, num_filters, kernel_size=3, batch_norm=True):\n",
        "    \n",
        "    x = layers.Activation('relu')(inputs)    \n",
        "\n",
        "    x = layers.Conv2DTranspose(filters=num_filters, \n",
        "                      kernel_size=(kernel_size, kernel_size),\n",
        "                      #kernel_initializer='he_normal', \n",
        "                      padding='same')(x)\n",
        "    \n",
        "    if batch_norm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Activation('relu')(x)    \n",
        "    \n",
        "    x = layers.Conv2DTranspose(filters=num_filters, \n",
        "                               kernel_size=(kernel_size, kernel_size),\n",
        "                               #kernel_initializer='he_normal', \n",
        "                               padding='same') (x)\n",
        "    if batch_norm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2KdeK5TPETx"
      },
      "source": [
        "def get_unet_model(img_size, num_classes):\n",
        "    inputs = tf.keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-ScZE9zQ4vp"
      },
      "source": [
        "tf.keras.backend.clear_session()\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH6tv153Q5Vh"
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXNLrSENPER5"
      },
      "source": [
        "img_size = (256,256)\n",
        "num_classes = 2\n",
        "\n",
        "#inputs = layers.Input(shape=img_size+(3,))\n",
        "unet = get_unet_model(img_size, num_classes)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gGH0UQaPEQS",
        "outputId": "80d49601-eba6-4a01-dbd3-599d256c6e18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "unet.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 128, 128, 32) 896         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 128, 128, 32) 128         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 128, 128, 32) 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 128, 128, 32) 0           activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_6 (SeparableCo (None, 128, 128, 64) 2400        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 128, 128, 64) 256         separable_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 128, 128, 64) 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_7 (SeparableCo (None, 128, 128, 64) 4736        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 128, 128, 64) 256         separable_conv2d_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 64)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 64, 64, 64)   2112        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 64, 64, 64)   0           max_pooling2d_3[0][0]            \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 64, 64, 64)   0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_8 (SeparableCo (None, 64, 64, 128)  8896        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 64, 64, 128)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_9 (SeparableCo (None, 64, 64, 128)  17664       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 128)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 128)  8320        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 32, 32, 128)  0           max_pooling2d_4[0][0]            \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 32, 32, 128)  0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_10 (SeparableC (None, 32, 32, 256)  34176       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 32, 32, 256)  1024        separable_conv2d_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 32, 32, 256)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_11 (SeparableC (None, 32, 32, 256)  68096       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 32, 32, 256)  1024        separable_conv2d_11[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 256)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 256)  33024       add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 16, 16, 256)  0           max_pooling2d_5[0][0]            \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 256)  0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTrans (None, 16, 16, 256)  590080      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 256)  1024        conv2d_transpose_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 256)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTrans (None, 16, 16, 256)  590080      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 256)  1024        conv2d_transpose_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_9 (UpSampling2D)  (None, 32, 32, 256)  0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_8 (UpSampling2D)  (None, 32, 32, 256)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 256)  65792       up_sampling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 32, 32, 256)  0           up_sampling2d_8[0][0]            \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 32, 32, 256)  0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_10 (Conv2DTran (None, 32, 32, 128)  295040      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 32, 32, 128)  512         conv2d_transpose_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 32, 32, 128)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_11 (Conv2DTran (None, 32, 32, 128)  147584      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 32, 32, 128)  512         conv2d_transpose_11[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_11 (UpSampling2D) (None, 64, 64, 256)  0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_10 (UpSampling2D) (None, 64, 64, 128)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 64, 64, 128)  32896       up_sampling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 64, 64, 128)  0           up_sampling2d_10[0][0]           \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 64, 64, 128)  0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DTran (None, 64, 64, 64)   73792       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 64, 64, 64)   256         conv2d_transpose_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 64, 64, 64)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DTran (None, 64, 64, 64)   36928       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 64, 64, 64)   256         conv2d_transpose_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_13 (UpSampling2D) (None, 128, 128, 128 0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_12 (UpSampling2D) (None, 128, 128, 64) 0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 128, 128, 64) 8256        up_sampling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 128, 128, 64) 0           up_sampling2d_12[0][0]           \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 128, 128, 64) 0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_14 (Conv2DTran (None, 128, 128, 32) 18464       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 128, 128, 32) 128         conv2d_transpose_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 128, 128, 32) 0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_15 (Conv2DTran (None, 128, 128, 32) 9248        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 128, 128, 32) 128         conv2d_transpose_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_15 (UpSampling2D) (None, 256, 256, 64) 0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_14 (UpSampling2D) (None, 256, 256, 32) 0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 256, 256, 32) 2080        up_sampling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 256, 256, 32) 0           up_sampling2d_14[0][0]           \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 256, 256, 2)  578         add_13[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,058,690\n",
            "Trainable params: 2,054,914\n",
            "Non-trainable params: 3,776\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w4JTKhalvMc",
        "outputId": "5641a8b4-2662-4a7e-fbc6-e4a903deed55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size = 32\n",
        "#batch_size = 16\n",
        "\n",
        "input_img_paths = sorted(metadata_train[\"sat_image_path\"])\n",
        "target_img_paths = sorted(metadata_train[\"mask_path\"])\n",
        "\n",
        "print('Number of training/validation samples:', len(input_img_paths))\n",
        "\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print('Satellite image:', input_path, '|', 'Road mask:', target_path)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training/validation samples: 6226\n",
            "Satellite image: /content/deep-globe/train/100034_sat.jpg | Road mask: /content/deep-globe/train/100034_mask.png\n",
            "Satellite image: /content/deep-globe/train/100081_sat.jpg | Road mask: /content/deep-globe/train/100081_mask.png\n",
            "Satellite image: /content/deep-globe/train/100129_sat.jpg | Road mask: /content/deep-globe/train/100129_mask.png\n",
            "Satellite image: /content/deep-globe/train/100703_sat.jpg | Road mask: /content/deep-globe/train/100703_mask.png\n",
            "Satellite image: /content/deep-globe/train/100712_sat.jpg | Road mask: /content/deep-globe/train/100712_mask.png\n",
            "Satellite image: /content/deep-globe/train/100773_sat.jpg | Road mask: /content/deep-globe/train/100773_mask.png\n",
            "Satellite image: /content/deep-globe/train/100841_sat.jpg | Road mask: /content/deep-globe/train/100841_mask.png\n",
            "Satellite image: /content/deep-globe/train/100867_sat.jpg | Road mask: /content/deep-globe/train/100867_mask.png\n",
            "Satellite image: /content/deep-globe/train/100892_sat.jpg | Road mask: /content/deep-globe/train/100892_mask.png\n",
            "Satellite image: /content/deep-globe/train/101225_sat.jpg | Road mask: /content/deep-globe/train/101225_mask.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDu4eoePPEMA"
      },
      "source": [
        "val_samples = int(0.2 * len(input_img_paths))\n",
        "\n",
        "random.Random(42).shuffle(input_img_paths)\n",
        "random.Random(42).shuffle(target_img_paths)\n",
        "\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "train_gen = SatDatClass(batch_size, img_size, train_input_img_paths, train_target_img_paths)\n",
        "val_gen = SatDatClass(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52zz5Am9PEGw",
        "outputId": "56b1edf5-f4e1-4a52-cb3f-28a2a4c821cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "unet.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(patience=8, verbose=1, \n",
        "                                           restore_best_weights=True),\n",
        "             tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, \n",
        "                                               verbose=1),\n",
        "             tf.keras.callbacks.ModelCheckpoint(\"/content/satellite_segmentation.h5\", \n",
        "                                             save_best_only=True)]\n",
        "\n",
        "epochs = 10\n",
        "unet.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 73/155 [=============>................] - ETA: 2:00 - loss: nan - accuracy: 0.9911"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSzQPT3PED4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd-yloicPEBK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9ra93J3PD-W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr6F6y2CPD7R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9ri4W5HPD4F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDCmDI8lPD1S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg-LJlJLEb2Y"
      },
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQf4EvNwEbyV"
      },
      "source": [
        "up_stack = [\n",
        "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "]\n",
        "\n",
        "def unet_model(output_channels:int):\n",
        "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(inputs)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      filters=output_channels, kernel_size=3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w2Mir2ZEbvj"
      },
      "source": [
        "OUTPUT_CLASSES = 2\n",
        "\n",
        "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-bJ635zEboU"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5kX067XEbej"
      },
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]), cmap='Greys_r')\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask\n",
        "\n",
        "def show_predictions(dataset=None, num=1):\n",
        "  if dataset:\n",
        "    #for image, mask in dataset.__getitem__(num):\n",
        "    image, mask = dataset.__getitem__(num)\n",
        "    pred_mask = model.predict(image[0].reshape(-1,128,128,3))\n",
        "    pred_mask = create_mask(pred_mask)\n",
        "    print(pred_mask.shape)\n",
        "    display([image[0], np.repeat(mask[0].reshape(128,128,1), 3, axis=2), np.repeat(pred_mask[0], 3, axis=2)])\n",
        "  else:\n",
        "    display([sample_image, sample_mask,\n",
        "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLSqJJ6WIgPO"
      },
      "source": [
        "x,y = train_gen.__getitem__(0)\n",
        "x.shape, x[0].shape, x[0].reshape(-1, 128, 128, 3).shape, y.shape, y[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzDfQgWdGiME"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "IMG_SIZE = (128, 128)\n",
        "input_img_paths = sorted(metadata_train[\"sat_image_path\"])\n",
        "target_img_paths = sorted(metadata_train[\"mask_path\"])\n",
        "\n",
        "# Split into training and validation set\n",
        "val_samples = int(0.2 * len(input_img_paths))\n",
        "random.Random(42).shuffle(input_img_paths)\n",
        "random.Random(42).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "train_gen = SatDatClass(BATCH_SIZE, IMG_SIZE, \n",
        "                        train_input_img_paths, train_target_img_paths)\n",
        "val_gen = SatDatClass(BATCH_SIZE, IMG_SIZE, \n",
        "                      val_input_img_paths, val_target_img_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwQ0kE63EapU"
      },
      "source": [
        "show_predictions(train_gen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjQCzms4Gt_H"
      },
      "source": [
        "EPOCHS = 1\n",
        "VAL_SUBSPLITS = 5\n",
        "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
        "\n",
        "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=test_batches,\n",
        "                          callbacks=[DisplayCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMHR-PBgGt8v"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.EarlyStopping(patience=8, verbose=1, \n",
        "                                           restore_best_weights=True),\n",
        "             tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, \n",
        "                                               verbose=1),\n",
        "             tf.keras.callbacks.ModelCheckpoint(\"/content/satellite_segmentation.h5\", \n",
        "                                             save_best_only=True)]\n",
        "\n",
        "# Train the model, validate at the end of each epoch\n",
        "EPOCHS = 5\n",
        "model.fit(train_gen, epochs=EPOCHS, \n",
        "          validation_data=val_gen, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zctt1cRfGt6W"
      },
      "source": [
        "#val_preds = model.predict(val_gen)\n",
        "show_predictions(val_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqFxUXLCGt39"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohjaWhPKGt1i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDbCw1FPGtyU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoqaXCCu5nDo"
      },
      "source": [
        "def normalize(input_image, input_mask):\n",
        "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  input_mask = input_mask\n",
        "  return input_image, input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_6UOtzX5n6C"
      },
      "source": [
        "def load_image(datapoint, img_size=IMG_SIZE):\n",
        "  img = plt.imread(datapoint['sat_image_path']) \n",
        "  input_image = tf.image.resize(img, img_size)\n",
        "\n",
        "  mask = plt.imread(datapoint['mask_path']) \n",
        "  input_mask = tf.image.resize(mask, img_size)\n",
        "\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mipEOa0y5n9s"
      },
      "source": [
        "VAL_FRAC = 0.2\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 1000\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
        "IMG_SIZE = (256, 256)\n",
        "\n",
        "# Split into training and validation set\n",
        "val_samples = int(VAL_FRAC * len(metadata_train))\n",
        "\n",
        "train_images = metadata_train.iloc[:-val_samples]\n",
        "train_images = train_images.progress_apply(load_image, axis=1)\n",
        "\n",
        "val_images = metadata_train.iloc[-val_samples:]\n",
        "val_images = val_images.progress_apply(load_image, axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8fxpx2g5oCX"
      },
      "source": [
        "class Augment(tf.keras.layers.Layer):\n",
        "  def __init__(self, seed=42):\n",
        "    super().__init__()\n",
        "    # both use the same seed, so they'll make the same randomn changes.\n",
        "    self.augment_inputs = preprocessing.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "    self.augment_labels = preprocessing.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "\n",
        "  def call(self, inputs, labels):\n",
        "    inputs = self.augment_inputs(inputs)\n",
        "    labels = self.augment_labels(labels)\n",
        "    return inputs, labels\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beNVDHW8C3ev"
      },
      "source": [
        "train_images[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLkG0wZaCxvu"
      },
      "source": [
        "train_images_test = pd.DataFrame([train_images[:][0], train_images[:][1]],columns=['sat_image', 'mask'])\n",
        "train_images_test.head(1)\n",
        "#train_images_test = tf.convert_to_tensor(train_images_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twre-wjB5oE7"
      },
      "source": [
        "train_batches = (\n",
        "    train_images\n",
        "    #.cache()\n",
        "    #.shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .repeat()\n",
        "    .map(Augment())\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "val_batches = val_images.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRJgb0lQ5oHx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoOol1DO5oKA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLb-xhc75oMs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElhUKmGk5mkv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zirF9lyKhL93"
      },
      "source": [
        "# Sample satellite images and road masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVImyM2MZPDE"
      },
      "source": [
        "img_idx = np.random.choice(range(len(metadata_train)), size=3)\n",
        "\n",
        "for i in img_idx:\n",
        "  sat_img = cv2.cvtColor(cv2.imread(metadata_train['sat_image_path'][i]), \n",
        "                         cv2.COLOR_BGR2RGB)\n",
        "  sat_mask = cv2.cvtColor(cv2.imread(metadata_train['mask_path'][i]), \n",
        "                          cv2.COLOR_BGR2RGB) / 255.\n",
        "\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "  fig.suptitle('Image ID: {}'.format(metadata_train['image_id'][i]), \n",
        "               fontsize=14)\n",
        "  ax[0].imshow(sat_img)\n",
        "  im = ax[1].imshow(sat_mask, cmap='Greys', vmin=0.0, vmax=1.0)\n",
        "\n",
        "  fig.subplots_adjust(right=0.8)\n",
        "  cbar_ax = fig.add_axes([0.85, 0.20, 0.05, 0.60])\n",
        "  fig.colorbar(im, cax=cbar_ax)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uprokJ3_f1Ke"
      },
      "source": [
        "img_dim, img_dim, img_depth = sat_mask.shape \n",
        "img_dim, img_dim, img_depth\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXHSWvjIhWNo"
      },
      "source": [
        "#img_size = (1024, 1024)\n",
        "img_size = (256, 256)\n",
        "#num_classes = 2\n",
        "num_classes = 1\n",
        "batch_size = 32\n",
        "#batch_size = 16\n",
        "\n",
        "input_img_paths = sorted(metadata_train[\"sat_image_path\"])\n",
        "target_img_paths = sorted(metadata_train[\"mask_path\"])\n",
        "\n",
        "print('Number of training/validation samples:', len(input_img_paths))\n",
        "\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print('Satellite image:', input_path, '|', 'Road mask:', target_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMcJLBLBnEYf"
      },
      "source": [
        "# Load and vectorize batches of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raabnQVOrRwQ"
      },
      "source": [
        "train_dict = {'img' : [], 'mask' : []}\n",
        "\n",
        "def load_data(load_dict=None, input_img_paths=None, target_img_paths=None, image_size=(128, 128)):\n",
        "    image_names = os.listdir(input_img_paths)\n",
        "    target_names = []\n",
        "\n",
        "    for name in image_names:\n",
        "        name = name.split('_')[0]\n",
        "        if name not in target_names:\n",
        "            target_names.append(name)\n",
        "    \n",
        "    image_dir = input_img_paths + '/'\n",
        "    target_dir = target_img_paths + '/'\n",
        "    \n",
        "    for i in range (len(image_names)):\n",
        "        try:\n",
        "            img = plt.imread(image_dir + target_names[i] + '_sat.jpg') \n",
        "            target = plt.imread(target_dir + target_names[i] + '_mask.png')\n",
        "            \n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        img = cv2.resize(img, image_size)\n",
        "        target = cv2.resize(target, image_size)\n",
        "\n",
        "        load_dict['img'].append(img)\n",
        "        load_dict['mask'].append(target[:,:,0])\n",
        "        \n",
        "    return load_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ljgrH1yjZqQ"
      },
      "source": [
        "class SatDatClass(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            #img = load_img(path, target_size=self.img_size)\n",
        "            img = plt.imread(path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            x[j] = img_to_array(img).astype(int) / 255\n",
        "        y = np.zeros((self.batch_size,) + self.img_size ,#+ (1,),\n",
        "                     dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            #img = load_img(path, target_size=self.img_size, \n",
        "                           #color_mode=\"grayscale\")\n",
        "            target = plt.imread(path)\n",
        "            target = cv2.resize(target, self.img_size)\n",
        "            y[j] = target[:, :, 0]\n",
        "            #y[j] = np.expand_dims(img, 2)\n",
        "            #y[j] = np.array(img)\n",
        "            #y[j] = np.expand_dims(img, 2).astype(int) // 255 #easier for network to interpret numbers in range [0,1]\n",
        "            # Ground truth labels are 1, 2, 3. \n",
        "            # Subtract one to make them 0, 1, 2:\n",
        "            #y[j] -= 1\n",
        "        \n",
        "        return x, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEBSkFN_mV5O"
      },
      "source": [
        "# U-Net Xception-style model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJdjSglPs9-u"
      },
      "source": [
        "def Conv2DBlock(inputs, previous_block_activation, num_filters, kernel_size=3, batch_norm=True):\n",
        "    \n",
        "    x = layers.Activation('relu')(x)    \n",
        "\n",
        "    #x = layers.Conv2D(filters=num_filters, \n",
        "    x = layers.SeparableConv2D(filters=num_filters, \n",
        "                      kernel_size=(kernel_size, kernel_size),\n",
        "                      #kernel_initializer='he_normal', \n",
        "                      padding='same')(inputs)\n",
        "    \n",
        "    if doBatchNorm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Activation('relu')(x)    \n",
        "    #x = layers.Activation('relu')(x)\n",
        "    \n",
        "    #x = tf.keras.layers.Conv2D(filters=num_filters, \n",
        "    x = layers.SeparableConv2D(filters=num_filters, \n",
        "                               kernel_size=(kernel_size, kernel_size),\n",
        "                               #kernel_initializer='he_normal', \n",
        "                               padding='same') (x)\n",
        "    if doBatchNorm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    #x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)    \n",
        "    #x = layers.Activation('relu')(x)\n",
        "\n",
        "    #residual = layers.Conv2D(num_filters, 1, strides=2, padding=\"same\")(\n",
        "            #previous_block_activation\n",
        "        #)\n",
        "    #x = layers.add([x, residual])  # Add back residual\n",
        "    #previous_block_activation = x\n",
        "    \n",
        "    return x#, residual, previous_block_activation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNL6DtKHq7jP"
      },
      "source": [
        "def Conv2DTransposeBlock(inputs, previous_block_activation, num_filters, kernel_size=3, batch_norm=True):\n",
        "    \n",
        "    x = layers.Activation('relu')(x)    \n",
        "\n",
        "    #x = layers.Conv2D(filters=num_filters, \n",
        "    x = layers.Conv2DTranspose(filters=num_filters, \n",
        "                      kernel_size=(kernel_size, kernel_size),\n",
        "                      #kernel_initializer='he_normal', \n",
        "                      padding='same')(inputs)\n",
        "    \n",
        "    if doBatchNorm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Activation('relu')(x)    \n",
        "    #x = layers.Activation('relu')(x)\n",
        "  \n",
        "    #x = layers.Conv2D(filters=num_filters, \n",
        "    x = layers.Conv2DTranspose(filters=num_filters, \n",
        "                      kernel_size=(kernel_size, kernel_size),\n",
        "                      #kernel_initializer='he_normal', \n",
        "                      padding='same')(inputs)\n",
        "    \n",
        "    if doBatchNorm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    #x = layers.UpSampling2D(2)(x)\n",
        "    \n",
        "    #x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)    \n",
        "    #x = layers.Activation('relu')(x)\n",
        "\n",
        "    #residual = layers.Conv2D(num_filters, 1, strides=2, padding=\"same\")(\n",
        "            #previous_block_activation\n",
        "        #)\n",
        "    #x = layers.add([x, residual])  # Add back residual\n",
        "    #previous_block_activation = x\n",
        "    \n",
        "    return x#, residual, previous_block_activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD6d3MhOuiBX"
      },
      "source": [
        "def get_unet_model(inputs, previous_block_activation, num_filters=16, dropout=0.1, batch_norm=True):\n",
        "    # encoder path\n",
        "    c1 = Conv2DBlock(inputs, previous_block_activation, num_filters*1, kernel_size=3, batch_norm=batch_norm)\n",
        "    p1 = layers.MaxPooling2D(3, strides=2, padding=\"same\")(c1)\n",
        "    # Project residual\n",
        "    residual = layers.Conv2D(num_filters*1, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
        "    p1 = layers.add([p1, residual])  # Add back residual\n",
        "    previous_block_activation = p1  # Set aside next residual\n",
        "    #p1 = tf.keras.layers.Dropout(droupouts)(p1)\n",
        "    \n",
        "    c2 = Conv2DBlock(p1, previous_block_activation, num_filters*2, kernel_size=3, batch_norm=batch_norm)\n",
        "    p2 = layers.MaxPooling2D(3, strides=2, padding=\"same\")(c2)\n",
        "    # Project residual\n",
        "    residual = layers.Conv2D(num_filters*2, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
        "    p2 = layers.add([p2, residual])  # Add back residual\n",
        "    previous_block_activation = p2  # Set aside next residual\n",
        "    #p2 = tf.keras.layers.Dropout(droupouts)(p2)\n",
        "    \n",
        "    c3 = Conv2DBlock(p2, previous_block_activation, num_filters*4, kernel_size=3, batch_norm=batch_norm)\n",
        "    p3 = layers.MaxPooling2D(3, strides=2, padding=\"same\")(c3)\n",
        "    # Project residual\n",
        "    residual = layers.Conv2D(num_filters*4, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
        "    p3 = layers.add([p3, residual])  # Add back residual\n",
        "    previous_block_activation = p3  # Set aside next residual\n",
        "    #p3 = tf.keras.layers.Dropout(droupouts)(p3)\n",
        "    \n",
        "    c4 = Conv2DBlock(p3, previous_block_activation, num_filters*8, kernel_size=3, batch_norm=batch_norm)\n",
        "    p4 = layers.MaxPooling2D(3, strides=2, padding=\"same\")(c4)\n",
        "    # Project residual\n",
        "    residual = layers.Conv2D(num_filters*2, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
        "    p4 = layers.add([p4, residual])  # Add back residual\n",
        "    previous_block_activation = p4  # Set aside next residual\n",
        "    #p4 = tf.keras.layers.Dropout(droupouts)(p4)\n",
        "    \n",
        "    c5 = Conv2DBlock(p4, previous_block_activation, num_filters*16, kernel_size=3, batch_norm=batch_norm)\n",
        "    '''\n",
        "    # defining decoder path\n",
        "    u6 = Conv2DTransposeBlock(num_filters*8, kernel_size=3, strides = (2, 2), padding = 'same')(u6)\n",
        "    # Project residual\n",
        "    residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "    residual = layers.Conv2D(num_filters*8, 1, padding=\"same\")(residual)\n",
        "    c6 = layers.add([u6, residual])  # Add back residual\n",
        "    previous_block_activation = x  # Set aside next residual\n",
        "    #u6 = layers.concatenate([u6, c4])\n",
        "    #u6 = layers.Dropout(droupouts)(u6)\n",
        "    #c6 = Conv2dBlock(u6, numFilters * 8, kernelSize = 3, doBatchNorm = doBatchNorm)\n",
        "    \n",
        "    u7 = tf.keras.layers.Conv2DTranspose(numFilters*4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
        "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "    u7 = tf.keras.layers.Dropout(droupouts)(u7)\n",
        "    c7 = Conv2dBlock(u7, numFilters * 4, kernelSize = 3, doBatchNorm = doBatchNorm)\n",
        "    \n",
        "    u8 = tf.keras.layers.Conv2DTranspose(numFilters*2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
        "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "    u8 = tf.keras.layers.Dropout(droupouts)(u8)\n",
        "    c8 = Conv2dBlock(u8, numFilters * 2, kernelSize = 3, doBatchNorm = doBatchNorm)\n",
        "    \n",
        "    u9 = tf.keras.layers.Conv2DTranspose(numFilters*1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
        "    u9 = tf.keras.layers.concatenate([u9, c1])\n",
        "    u9 = tf.keras.layers.Dropout(droupouts)(u9)\n",
        "    c9 = Conv2dBlock(u9, numFilters * 1, kernelSize = 3, doBatchNorm = doBatchNorm)\n",
        "    \n",
        "    output = tf.keras.layers.Conv2D(1, (1, 1), activation = 'sigmoid')(c9)\n",
        "    model = tf.keras.Model(inputs = [inputImage], outputs = [output])\n",
        "    '''\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db0jbt4RvG2r"
      },
      "source": [
        "inputs = layers.Input(shape=img_size+(3,))\n",
        "\n",
        "# Entry block\n",
        "x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "previous_block_activation = x  # Set aside residual\n",
        "\n",
        "unet = get_unet_model(inputs, previous_block_activation, num_filters=16)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79eeJNkIkm_U"
      },
      "source": [
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "    print(inputs.shape)\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", \n",
        "                            padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsVNA6QNmSMI"
      },
      "source": [
        "# Free up RAM in case the model definition cells were run multiple times\n",
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMzZHbUFmTqj"
      },
      "source": [
        "model = get_model(img_size, num_classes)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWIdaEdOmhYw"
      },
      "source": [
        "# Split into training and validation set\n",
        "val_samples = int(0.2 * len(input_img_paths))\n",
        "random.Random(42).shuffle(input_img_paths)\n",
        "random.Random(42).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "train_gen = SatDatClass(batch_size, img_size, \n",
        "                        train_input_img_paths, train_target_img_paths)\n",
        "val_gen = SatDatClass(batch_size, img_size, \n",
        "                      val_input_img_paths, val_target_img_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IaiOWbfPXxx"
      },
      "source": [
        "val_gen.__len__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBndN1h-3Nf5"
      },
      "source": [
        "x,y = train_gen.__getitem__(0)\n",
        "print(x.min(), x.max())\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "fig.suptitle('Image ID: {}'.format(metadata_train['image_id'][0]), \n",
        "               fontsize=14)\n",
        "ax[0].imshow(np.array(x[4]))\n",
        "ax[1].imshow(np.array(y[4]), cmap='Greys');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWgPa4hJnTlj"
      },
      "source": [
        "#model.compile(optimizer=\"rmsprop\", \n",
        "              #loss=\"sparse_categorical_crossentropy\", \n",
        "              #metrics=['accuracy'],)#[keras.metrics.SparseCategoricalAccuracy()])#\n",
        "                       #,keras.metrics.Precision(),\n",
        "                       #keras.metrics.Recall(),\n",
        "                       #keras.metrics.MeanIoU(num_classes=num_classes)],)\n",
        "model.compile(#optimizer='rmsprop', \n",
        "              optimizer='adam', \n",
        "              #loss='sparse_categorical_crossentropy', \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['accuracy'],)\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=8, verbose=1, \n",
        "                                           restore_best_weights=True),\n",
        "             keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, \n",
        "                                               verbose=1),\n",
        "             keras.callbacks.ModelCheckpoint(\"/content/satellite_segmentation.h5\", \n",
        "                                             save_best_only=True)]\n",
        "\n",
        "# Train the model, validate at the end of each epoch\n",
        "epochs = 1\n",
        "model.fit(train_gen, epochs=epochs, \n",
        "          validation_data=val_gen, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU1q01pgnlYJ"
      },
      "source": [
        "#val_gen = SatDatClass(batch_size, img_size, \n",
        "                      #val_input_img_paths, val_target_img_paths)\n",
        "val_preds = model.predict(val_gen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuUI0TTlWO7B"
      },
      "source": [
        "i = 18\n",
        "\n",
        "val_preds[i].shape, val_preds[i].min(), val_preds[i].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7Y_Iyj_xywU"
      },
      "source": [
        "i = 18\n",
        "\n",
        "sat_img = cv2.cvtColor(cv2.imread(val_input_img_paths[i]), \n",
        "                         cv2.COLOR_BGR2RGB)\n",
        "sat_mask = cv2.cvtColor(cv2.imread(val_target_img_paths[i]), \n",
        "                          cv2.COLOR_BGR2RGB)[:,:,0]\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
        "\n",
        "fig.suptitle('Image ID: {}'.format(metadata_train['image_id'][i]), \n",
        "               fontsize=14)\n",
        "ax[0].imshow(sat_img)\n",
        "ax[1].imshow(sat_mask, cmap='Greys')\n",
        "\n",
        "mask = val_preds[i][:, :, 0]# < 0.9 #np.argmax(val_preds[i], axis=-1)\n",
        "mask = np.expand_dims(mask, axis=-1)\n",
        "ax[2].imshow(keras.preprocessing.image.array_to_img(mask), cmap='Greys');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWqYM2r7FZpl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWYZKwOBFZZs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0gX5VCtFYyt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toYCoz-e_7A6"
      },
      "source": [
        "x = val_preds[:][:, :, 0]\n",
        "\n",
        "x.min(), x.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlCiP9Dh9mwG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVjBE3xU1OEQ"
      },
      "source": [
        "img = load_img(metadata_train['sat_image_path'][0], target_size=img_size)\n",
        "msk = load_img(metadata_train['mask_path'][0], target_size=img_size,\n",
        "                           color_mode=\"grayscale\")\n",
        "y = np.expand_dims(msk, 2) / 255.\n",
        "np.array(img).shape, np.array(msk).shape, np.array(msk).min(), np.array(msk).max(), y.shape, y.min(), y.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7m8Ot-g19TB"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "fig.suptitle('Image ID: {}'.format(metadata_train['image_id'][0]), \n",
        "               fontsize=14)\n",
        "ax[0].imshow(img)\n",
        "ax[1].imshow(np.array(msk), cmap='Greys');\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}